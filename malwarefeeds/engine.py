# -*- coding: utf-8 -*-
"""An Engine to handle the malware feeds."""

import re
from typing import Tuple

from requests import get


class Engine(object):
    """An Engine to handle the malware feeds."""

    def __init__(self):
        """Initialize the Engine object."""
        self.malcode = []
        self.mdl = []
        self.vxvault = []
        self.ransomware = []
        self._headers = {'User-agent': 'Mozilla/5.0 (X11; U; Linux i686)'}
        self._feeds = {
            'malcode': {
                'url': 'http://malc0de.com/rss',
                'parser': self._parse_xml_list_desc,
                'body': b''
            },
            'mdl': {
                'url': 'http://www.malwaredomainlist.com/hostslist/mdl.xml',
                'parser': self._parse_xml_list_desc,
                'body': b''
            },
            'vxvault': {
                'url': 'http://vxvault.net/URL_List.php',
                'parser': self._parse_txt_file,
                'body': b''
            },
            'ransomware': {
                'url': 'https://ransomwaretracker.abuse.ch/downloads/RW_URLBL.txt',
                'parser': self._parse_xml_list_desc,
                'body': b''
            }
        }

    def download(self):
        """
        Fetch the content from all feeds.

        raise: ConnectionError
        """
        for feed in self._feeds.keys():
            url = self._feeds.get(feed).get('url')
            body = get(url, headers=self._headers).content
            self._feeds.get(feed)['body'] = body

    def get_urls(self) -> Tuple[str, str]:
        """Get the urls from feeds."""
        for feed in self._feeds.keys():
            body = self._feeds.get(feed).get('body')
            parser = self._feeds.get(feed).get('parser')
            for url in parser(str(body, 'utf-8')):
                yield feed, url

    def _parse_xml_list_desc(self, response: str):
        """Parse a XML Feed."""
        if not response or len(response) == 0:
            return
        rex = r'(<description>)(URL: |Host: )(?P<URL>.*)(, IP.*<\/description>)'
        matches = re.finditer(rex, response, re.MULTILINE)
        for match in matches:
            url = match.groups()[2]
            yield url

    def _parse_txt_file(self, response: str, splitter: str = '\r\n'):
        """Parse text files and split it."""
        if response:
            if splitter not in response:
                splitter = '\n'
            for url in response.split(splitter):
                if url.startswith('http'):
                    yield url
